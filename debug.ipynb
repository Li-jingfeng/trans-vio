{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([81920, 2, 511])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.ones(81920,2,512)\n",
    "y = torch.ones(1,2,512)\n",
    "x[:,:,1:].shape\n",
    "# z = x+y\n",
    "# print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "a0 = torch.zeros(2,1,3,3)\n",
    "a1 = torch.ones(2,1,3,3)\n",
    "\n",
    "input = torch.cat([a0,a1],dim=1)\n",
    "out1 = rearrange(input, 'b t n d -> b (t n) d')\n",
    "out2 = rearrange(out1, 'b (t n) d -> b t n d',t=2,n=3)\n",
    "\n",
    "print('input',input)\n",
    "# print('out1',out1)\n",
    "print('out2',out2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "pre = None\n",
    "a = None\n",
    "if pre and a is None:\n",
    "    print(1)\n",
    "else:\n",
    "    print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pose_abs, pose_rel = read_pose_from_text(\"/data/lijingfeng/pro/Visual-Selective-VIO/data/poses/05.txt\")\n",
    "pose_abs = np.array(pose_abs)\n",
    "pose_rel = np.array(pose_rel)\n",
    "x = pose_abs[:,0,3]\n",
    "y = pose_abs[:,1,3]\n",
    "z = pose_abs[:,2,3]\n",
    "\n",
    "#画图\n",
    "style_gt = 'r-'\n",
    "style_O = 'ko'\n",
    "plot_keys = [\"Ground Truth\", \"Ours\"]\n",
    "plt.plot(x, z, style_gt, label=plot_keys[0])\n",
    "start_point = [0, 0]\n",
    "ax = plt.gca()\n",
    "plt.plot(start_point[0], start_point[1], style_O, label='Start Point')\n",
    "plt.legend(loc=\"upper right\", prop={'size': 10})\n",
    "plt.xlabel('x (m)', fontsize=10)\n",
    "plt.ylabel('z (m)', fontsize=10)\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "xmean = np.mean(xlim)\n",
    "ymean = np.mean(ylim)\n",
    "plot_radius = max([abs(lim - mean_)\n",
    "                   for lims, mean_ in ((xlim, xmean),\n",
    "                                       (ylim, ymean))\n",
    "                   for lim in lims])\n",
    "ax.set_xlim([xmean - plot_radius, xmean + plot_radius])\n",
    "ax.set_ylim([ymean - plot_radius, ymean + plot_radius])\n",
    "plt.title('2D path')\n",
    "plt.savefig(\"/data/lijingfeng/pro/Visual-Selective-VIO/seq05gt.png\", bbox_inches='tight', pad_inches=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rgb': PatchedInputAdapter(), 'depth': PatchedInputAdapter()}\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from pointnav_vo.mmae import *\n",
    "DOMAIN_CONF = {\n",
    "                'rgb': {\n",
    "                    'input_adapter': partial(PatchedInputAdapter, num_channels=3, stride_level=1),\n",
    "                    'output_adapter': partial(PatchedOutputAdapterXA, num_channels=3, stride_level=1),\n",
    "                    'loss': MaskedMSELoss,\n",
    "                },\n",
    "                'depth': {\n",
    "                    'input_adapter': partial(PatchedInputAdapter, num_channels=1, stride_level=1),\n",
    "                    'output_adapter': partial(PatchedOutputAdapterXA, num_channels=1, stride_level=1),\n",
    "                    'loss': MaskedMSELoss,\n",
    "                },\n",
    "                'semseg': {\n",
    "                    'input_adapter': partial(SemSegInputAdapter, num_classes=133,\n",
    "                                            dim_class_emb=64, interpolate_class_emb=False, stride_level=4),\n",
    "                    'output_adapter': partial(PatchedOutputAdapterXA, num_channels=133, stride_level=4),\n",
    "                    'loss': MaskedCrossEntropyLoss,\n",
    "                },\n",
    "            }\n",
    "input_adapters = {\n",
    "    domain: dinfo['input_adapter'](\n",
    "        patch_size_full = 16,\n",
    "    )\n",
    "    for domain, dinfo in DOMAIN_CONF.items()\n",
    "    if domain in ['rgb','depth']\n",
    "}\n",
    "print(input_adapters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.LayerNorm(1,2,3)\n",
    "input = torch.randn(2,3).to('cuda:0')\n",
    "model.cuda(1)\n",
    "\n",
    "out = model(input)\n",
    "# prove model and input tensor must be the same device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型一般随机初始化，现在将其参数初始化为全0\n",
    "# 将模型参数初始化为0\n",
    "from vo_transformer import VisualOdometryTransformerActEmbed\n",
    "model = VisualOdometryTransformerActEmbed(obs_size_single=(192,341),cls_action=False)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from vo_transformer import VisualOdometryTransformerActEmbed\n",
    "\n",
    "model = VisualOdometryTransformerActEmbed(obs_size_single=(192,341),cls_action=False)\n",
    "# 权重参数的keys\n",
    "model_state = torch.load('./model_zoo/fix_model.pth')\n",
    "print(model_state[-1].keys())\n",
    "# 模型的keys\n",
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将cvpr的模型加载到这个框架下，先是将所估计的seq5的结果保存到txt文件中（x,z,yaw）\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from vo_transformer import VisualOdometryTransformerActEmbed\n",
    "\n",
    "model = VisualOdometryTransformerActEmbed(cls_action=False)\n",
    "model.load_state_dict(torch.load('./model_zoo/remove_module_fix_model.pth'))\n",
    "\n",
    "# 保存直接使用test.py中的代码，修改eval中的部分代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVIO中的pose转换代码\n",
    "import numpy as np\n",
    "import math\n",
    "_EPS = np.finfo(float).eps * 4.0\n",
    "\n",
    "def read_pose(line):\n",
    "    '''\n",
    "    Reading 4x4 pose matrix from .txt files\n",
    "    input: a line of 12 parameters\n",
    "    output: 4x4 numpy matrix\n",
    "    '''\n",
    "    values= np.reshape(np.array([float(value) for value in line.split(' ')]), (3, 4))\n",
    "    Rt = np.concatenate((values, np.array([[0, 0, 0, 1]])), 0)\n",
    "    return Rt\n",
    "\n",
    "def get_relative_pose(Rt1, Rt2):\n",
    "    '''\n",
    "    Calculate the relative 4x4 pose matrix between two pose matrices\n",
    "    '''\n",
    "    Rt1_inv = np.linalg.inv(Rt1)\n",
    "    Rt_rel = Rt1_inv @ Rt2\n",
    "    return Rt_rel\n",
    "\n",
    "def euler_from_matrix(matrix):\n",
    "    '''\n",
    "    Extract the eular angle from a rotation matrix\n",
    "    '''\n",
    "    M = np.array(matrix, dtype=np.float64, copy=False)[:3, :3]\n",
    "    cy = math.sqrt(M[0, 0] * M[0, 0] + M[1, 0] * M[1, 0])\n",
    "    ay = math.atan2(-M[2, 0], cy)\n",
    "    if ay < -math.pi / 2 + _EPS and ay > -math.pi / 2 - _EPS:  # pitch = -90 deg\n",
    "        ax = 0\n",
    "        az = math.atan2(-M[1, 2], -M[0, 2])\n",
    "    elif ay < math.pi / 2 + _EPS and ay > math.pi / 2 - _EPS:\n",
    "        ax = 0\n",
    "        az = math.atan2(M[1, 2], M[0, 2])\n",
    "    else:\n",
    "        ax = math.atan2(M[2, 1], M[2, 2])\n",
    "        az = math.atan2(M[1, 0], M[0, 0])\n",
    "    return np.array([ax, ay, az])\n",
    "\n",
    "def get_relative_pose_6DoF(Rt1, Rt2):\n",
    "    '''\n",
    "    Calculate the relative rotation and translation from two consecutive pose matrices \n",
    "    '''\n",
    "    \n",
    "    # Calculate the relative transformation Rt_rel\n",
    "    Rt_rel = get_relative_pose(Rt1, Rt2)\n",
    "\n",
    "    R_rel = Rt_rel[:3, :3]\n",
    "    t_rel = Rt_rel[:3, 3]\n",
    "\n",
    "    # Extract the Eular angle from the relative rotation matrix\n",
    "    x, y, z = euler_from_matrix(R_rel)\n",
    "    theta = [x, y, z]\n",
    "\n",
    "    pose_rel = np.concatenate((t_rel, theta))\n",
    "    return pose_rel\n",
    "\n",
    "def read_pose_from_text(path):\n",
    "    with open(path) as f:\n",
    "        lines = [line.split('\\n')[0] for line in f.readlines()]\n",
    "        poses_rel, poses_abs = [], []\n",
    "        values_p = read_pose(lines[0])\n",
    "        poses_abs.append(values_p)            \n",
    "        for i in range(1, len(lines)):\n",
    "            values = read_pose(lines[i])\n",
    "            poses_rel.append(get_relative_pose_6DoF(values_p, values)) \n",
    "            values_p = values.copy()\n",
    "            poses_abs.append(values) \n",
    "        poses_abs = np.array(poses_abs)\n",
    "        poses_rel = np.array(poses_rel)\n",
    "\n",
    "    return poses_abs, poses_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取kitti中seq5的数据，只要x,y,yaw数据，保存为gt\n",
    "import numpy as np\n",
    "\n",
    "file_path = \"./data/poses/04.txt\"\n",
    "file_save_path = \"gt_seq4_rel_pose_3dof.txt\"\n",
    "\n",
    "poses_abs, poses_rel = read_pose_from_text(file_path)\n",
    "np.savetxt(file_save_path, np.array(poses_rel)[:,[0,2,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_head 输出rel_pose是6DoF，[theta,xyz]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 然后将这个txt文件读取出来，然后将其转换为pose_abs和pose_rel 2维坐标系下\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def pose_3DoF_to_matrix(pose):\n",
    "    '''\n",
    "    Calculate the 3x3 transformation matrix from Eular angles and translation vector\n",
    "    '''\n",
    "    delta_yaw_rad = pose[2]\n",
    "    R = np.array([[math.cos(delta_yaw_rad), -math.sin(delta_yaw_rad)],\n",
    "                  [math.sin(delta_yaw_rad), math.cos(delta_yaw_rad)]])\n",
    "    t = T = np.array([[pose[0]],\n",
    "                  [pose[1]]])\n",
    "    R = np.concatenate((R, t), 1)\n",
    "    R = np.concatenate((R, np.array([[0, 0, 1]])), 0)\n",
    "\n",
    "    return R\n",
    "\n",
    "def pose_accu(Rt_pre, R_rel):\n",
    "    '''\n",
    "    Calculate the accumulated pose from the latest pose and the relative rotation and translation\n",
    "    '''\n",
    "    Rt_rel = pose_3DoF_to_matrix(R_rel)\n",
    "    return Rt_pre @ Rt_rel\n",
    "\n",
    "def path_accu(pose):\n",
    "    '''\n",
    "    Generate the global pose matrices from a series of relative poses\n",
    "    '''\n",
    "    answer = [np.eye(3)]\n",
    "    for index in range(pose.shape[0]):\n",
    "        pose_ = pose_accu(answer[-1], pose[index, :])\n",
    "        answer.append(pose_)\n",
    "    return answer\n",
    "\n",
    "# relative_poses = np.loadtxt('gt_seq4_rel_pose_3dof.txt')  # 假设poses.txt包含x,z,yaw的相对姿态数据\n",
    "relative_poses = np.loadtxt('cvpr_seq4_rel_pose_3dof.txt')  # 假设poses.txt包含x,z,yaw的相对姿态数据\n",
    "\n",
    "global_poses = path_accu(relative_poses)\n",
    "pose_3dof = np.array(global_poses).reshape(-1,1,9)\n",
    "\n",
    "# np.savetxt(\"gt_seq4_global_pose_3dof.txt\", pose_3dof.squeeze(1)[:,[2,5]])\n",
    "np.savetxt(\"cvpr_seq4_global_pose_3dof.txt\", pose_3dof.squeeze(1)[:,[2,5]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatgpt给出的示例，不对\n",
    "def relative_to_global_pose(x_rel, z_rel, yaw_rel, x_ref, z_ref, yaw_ref):\n",
    "    # delta_yaw_rad = math.radians(yaw_rel)\n",
    "    # 本来就是弧度\n",
    "    delta_yaw_rad = yaw_rel\n",
    "    R = np.array([[math.cos(delta_yaw_rad), -math.sin(delta_yaw_rad)],\n",
    "                  [math.sin(delta_yaw_rad), math.cos(delta_yaw_rad)]])\n",
    "    T = np.array([[x_rel],\n",
    "                  [z_rel]])\n",
    "    global_pose = np.dot(R, np.array([[x_ref], [z_ref]])) + T\n",
    "\n",
    "    return x_global, z_global, yaw_global\n",
    "\n",
    "# 从文本文件中读取相对姿态数据\n",
    "relative_poses = np.loadtxt('gt_seq5_rel_pose_3dof.txt')  # 假设poses.txt包含x,z,yaw的相对姿态数据\n",
    "# relative_poses = np.loadtxt('cvpr_seq5_rel_pose_3dof.txt')  # 假设poses.txt包含x,z,yaw的相对姿态数据\n",
    "\n",
    "# 假设参考全局姿态为 (x_ref, z_ref, yaw_ref)\n",
    "x_ref = 0\n",
    "z_ref = 0\n",
    "yaw_ref = 0\n",
    "print(\"坐标系原点：\", x_ref,'  ',z_ref,'  ', yaw_ref, \"rad\")\n",
    "\n",
    "# 转换相对姿态为全局姿态\n",
    "global_poses = []\n",
    "for relative_pose in relative_poses:\n",
    "    x_rel, z_rel, yaw_rel = relative_pose\n",
    "    x_global, z_global, yaw_global = relative_to_global_pose(x_rel, z_rel, yaw_rel, x_ref, z_ref, yaw_ref)\n",
    "    global_poses.append([x_global, z_global, yaw_global])\n",
    "    x_ref = x_global\n",
    "    z_ref = z_global\n",
    "    yaw_ref = yaw_global\n",
    "\n",
    "# 打印全局姿态数据\n",
    "for global_pose in global_poses:\n",
    "    # print(global_pose)\n",
    "    # np.savetxt(\"cvpr_seq5_global_pose_3dof.txt\", np.array(global_poses))\n",
    "    np.savetxt(\"gt_seq5_global_pose_3dof.txt\", np.array(global_poses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 2D path图，只需要用到x,z坐标\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotPath_2D(seq, poses_gt_mat, poses_est_mat, plot_path_dir):\n",
    "\n",
    "    fontsize_ = 10\n",
    "    plot_keys = [\"Ground Truth\", \"Ours\"]\n",
    "    start_point = [0, 0]\n",
    "    style_pred = 'b-'\n",
    "    style_gt = 'r-'\n",
    "    style_O = 'ko'\n",
    "\n",
    "    # get the value\n",
    "    x_gt = [pose[0] for pose in poses_gt_mat]\n",
    "    z_gt = [pose[1] for pose in poses_gt_mat]\n",
    "\n",
    "    x_pred = [pose[0] for pose in poses_est_mat]\n",
    "    z_pred = [pose[1] for pose in poses_est_mat]\n",
    "\n",
    "    # Plot 2d trajectory estimation map\n",
    "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
    "    ax = plt.gca()\n",
    "    plt.plot(x_gt, z_gt, style_gt, label=plot_keys[0])\n",
    "    plt.plot(x_pred, z_pred, style_pred, label=plot_keys[1])\n",
    "    plt.plot(start_point[0], start_point[1], style_O, label='Start Point')\n",
    "    plt.legend(loc=\"upper right\", prop={'size': fontsize_})\n",
    "    plt.xlabel('x (m)', fontsize=fontsize_)\n",
    "    plt.ylabel('z (m)', fontsize=fontsize_)\n",
    "    # set the range of x and y\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    xmean = np.mean(xlim)\n",
    "    ymean = np.mean(ylim)\n",
    "    plot_radius = max([abs(lim - mean_)\n",
    "                       for lims, mean_ in ((xlim, xmean),\n",
    "                                           (ylim, ymean))\n",
    "                       for lim in lims])\n",
    "    ax.set_xlim([xmean - plot_radius, xmean + plot_radius])\n",
    "    ax.set_ylim([ymean - plot_radius, ymean + plot_radius])\n",
    "\n",
    "    plt.title('2D path')\n",
    "    png_title = \"{}_path_2d\".format(seq)\n",
    "    plt.savefig(plot_path_dir + \"/\" + png_title + \".png\", bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用plotPath_2D\n",
    "import numpy as np\n",
    "seq = '04'\n",
    "poses_gt_mat = np.loadtxt(\"gt_seq4_global_pose_3dof.txt\")\n",
    "poses_est_mat = np.loadtxt(\"cvpr_seq4_global_pose_3dof.txt\")\n",
    "plot_path_dir = \"./fig\"\n",
    "\n",
    "plotPath_2D(seq, poses_gt_mat, poses_est_mat, plot_path_dir)\n",
    "# rel_poses -> abs_poses 所有的abs_poses都是相对于第一个pose（这不对）\n",
    "# 应该是得到上一帧的abs_pose，然后通过相对pose去变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算rel pose的误差\n",
    "def rmse_err_cal(pose_est, pose_gt):\n",
    "    '''\n",
    "    Calculate the rmse of relative translation and rotation\n",
    "    '''\n",
    "    t_rmse = np.sqrt(np.mean(np.sum((pose_est[:, :2] - pose_gt[:, :2])**2, -1)))\n",
    "    r_rmse = np.sqrt(np.mean(np.sum((pose_est[:, 2] - pose_gt[:, 2])**2, -1)))\n",
    "    return t_rmse, r_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_rmse:  6.636896475837878 r_rmse:  81.98909829795181\n"
     ]
    }
   ],
   "source": [
    "pose_est = np.loadtxt(\"cvpr_seq4_rel_pose_3dof.txt\")\n",
    "pose_gt = np.loadtxt(\"gt_seq4_rel_pose_3dof.txt\")\n",
    "t_rmse, r_rmse = rmse_err_cal(pose_est, pose_gt)\n",
    "print(\"t_rmse: \", t_rmse, \"r_rmse: \", r_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_est = pose_est[:,[3,5,1]]\n",
    "print(pose_est)\n",
    "t_rmse, r_rmse = rmse_err_cal(pose_est, pose_gt)\n",
    "print(\"t_rmse: \", t_rmse, \"r_rmse: \", r_rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SVIO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
